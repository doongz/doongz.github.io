# 虚拟化

[深入理解虚拟化](https://zhuanlan.zhihu.com/p/441287815)

[Docker，K8s，KVM，Hypervisor和微服务有什么区别联系吗？](https://www.zhihu.com/question/307537564)

[虚拟化技术的分类及介绍](https://blog.csdn.net/mumuriyue/article/details/85714900)

## docker和虚拟化的区别

首先要说的是题目隐含的一个概念：**硬件虚拟化。**硬件虚拟化主要来源于两个前提条件：

- 硬件性能过剩。这个就不详细解释了，消费级电脑的主流配置已经6核的今天，除了3A游戏以及例如视频制作、3D渲染、高性能计算之类的专业应用，大部分电脑的CPU性能闲置率超过90%。
- 业务需要多台主机——这个需求一部分来源于软件冲突，或者版本冲突；另一部分来源于需要不同的操作系统（如果把操作系统看做一个独立的软件，也可以归类为软件冲突）。服务器上的先不说，如果是做web前端的，现在好一点，以前经常要测试网页在不同版本的IE上是否能够正常浏览，然而一台电脑又只能装一个版本的IE，这就是软件版本冲突。

配备多台电脑固然是一个解决方案，但显然成本过高。因此就有硬件虚拟化的提出——用软件仿真一台电脑出来，在这台虚拟出来的电脑上安装操作系统，配置网络，安装运行软件等等。很多人听说过用过的VMware就这么一个软件。这个能够虚拟出一台或者多台电脑，并对这些虚拟机进行管理的软件，就是Hypervisor。

知道VMware的大部分人都听说过两个版本：一个是VMware Workstation，一个是VMware ESXi。这两个版本最大的区别，就是Workstation是运行在某个已有操作系统上的软件，例如Windows或者Linux，由操作系统实现对硬件资源的访问。而ESXi则本身就是一个操作系统，直接运行在硬件之上，省去了Host OS的开销。这就是两种不同的Hypervisor。

```
                                        +-----+-----+-----+-----+-----+          
                                        |App A|App B|App C|App D|App E|          
+-----+-----+-----+-----+-----+         +-----+-----+-----+-----+-----+           
|App A|App B|App C|App D|App E|         |Guest|Guest|Guest|Guest|Guest|           
+-----+-----+-----+-----+-----+         |OS A |OS B |OS C |OS D |OS E |           
|Guest|Guest|Guest|Guest|Guest|         +-----+-----+-----+-----+-----+           
|OS A |OS B |OS C |OS D |OS E |         |         Hypervisor          |           
+-----+-----+-----+-----+-----+         +-----------------------------+           
|         Hypervisor          |         |          Host OS            |           
+-----------------------------+         +-----------------------------+           
|          Hardware           |         |          Hardware           |           
+-----------------------------+         +-----------------------------+            
            Type I                                  Type Ⅱ
```

ESXi属于Type I，Workstation属于Type II。Windows自带的Hyper-V，Linux上的Xen也属于Type I，Virtual Box之类的属于Type II。

而Linux从2.6.20开始，就从内核支持虚拟化，可以理解为内核就是Hypervisor的一部分，配合Qemu实现完整的Hypervisor功能。所以叫“**基于内核的虚拟机**（英语：**K**ernel-based **V**irtual **M**achine，缩写为**KVM**）”[1]，不过因为内核部分跟近似于Type I但又不是完整的Hypervisor，Qemu部分则更接近于Type II，所以在维基的Hyperviso词条的Talk页面中，不少人为KVM属于Type I还是Type II而激烈争论。

虚拟机有一个问题，就是虚拟机里面的操作系统本身会占用相当一部分资源。例如最新版的Windows 10，安装完毕什么都不干，就占用20~30G的硬盘空间，2~3G的内存。即使是只有字符终端没有图形界面的Linux，根据发行版和安装软件的不同，往往也需要100M~1G的内存，1~10G的硬盘空间。如果说同时运行2~3台还能接受这个开销的话，同时运行数十台，这个开销就不是少数了。事实上，往往这数十台虚拟机运行的都是相同的操作系统，只是根据业务需求的不同，安装的软件，或者软件版本不同而已。因此，业界提出了操作系统虚拟化（也叫容器，Containerization）的概念——所有的虚拟机使用同一个操作系统内核。容器和Type II的Hypervisor相比，最大的区别就是省去了客户机操作系统的开销，如下图：

```
+-----+-----+-----+-----+-----+           
|App A|App B|App C|App D|App E|           
+-----+-----+-----+-----+-----+           
|Guest|Guest|Guest|Guest|Guest|           +-----+-----+-----+-----+-----+
|OS A |OS B |OS C |OS D |OS E |           |App A|App B|App C|App D|App E|
+-----+-----+-----+-----+-----+           +-----+-----+-----+-----+-----+
|         Hypervisor          |           |      Container Engine       |
+-----------------------------+           +-----------------------------+
|          Host OS            |           |          Host OS            |
+-----------------------------+           +-----------------------------+
|          Hardware           |           |          Hardware           |
+-----------------------------+           +-----------------------------+
      Type II Hypervisor                          Containerization
```

容器技术有很多种，最常见的就是题目所说的Docker，此外还有Linux的LXC、OpenVZ，FreeBSD的Jails，Solaris的Zones等等。值得一提的是各种Unix的chroot命令，也可以看做是一种特殊的容器。

当然，Docker除了实现容器以外，还有相关的一系列概念和组件，例如镜像、仓库等等，大大简化了应用的分发、部署等工作，这些是Docker得以流行的很重要的因素，但这里就不展开了。有兴趣再了解这方面的请参考我的另外一个回答：[如何通俗解释Docker是什么？](https://www.zhihu.com/question/28300645/answer/585166942)

我们知道，因为性能、安全、容错等等原因，有时候我们需要把若干台物理机组成一个集群来提供服务。同样的，虚拟机和容器也有这样的需求，而容器的集群管理，目前比较流行的就是Kubernetes（简称K8s）。K8s把一个或者多个实现某种业务的容器，组成一个Pod，然后对这些Pods进行监控和管理。

最后，微服务应该算是业务层面的。一般来说，每个完整的业务系统，都可以分为若干个组件，举例来说，我们做一个学校的选课系统，通常可以划分为课程管理、教师管理、学生管理、教室管理以及选课系统这几个耦合比较松散的组件。如果我们把每个组件作为一个独立的系统进行开发，并且对其它系统提供业务接口（服务），那么每个这样的独立系统，就是一个微服务。将来进行业务扩展的时候这些微服务就可以方便的得以复用，例如学生管理服务可以为学籍管理系统提供学生信息服务，教师管理服务可以向学校的OA、财务系统提供教职工信息服务，……，等等。另外，也可以根据业务的不同特点，或者开发小组/团队的技术实力，使用不同的开发技术、架构进行开发。

这些一个个的微服务，可能相当大部分只需要使用很少的硬件资源，但微服务的数量又很多，把一个微服务部署在一个或者多个容器的组合里面就最适合不过了。而每一个微服务，作为K8s的一个Pod，统一进行管理自然也是水到渠成的方案。

## 一、什么是虚拟化

虚拟化技术是云计算的根基，在计算机技术中，**虚拟化（技术）**或**虚拟技术**（英语：Virtualization）是一种资源管理技术，是将计算机的各种实体资源（**CPU、内存、磁盘空间、网络适配器**等），予以抽象、转换后呈现出来并可供分割、组合为一个或多个电脑配置环境。由此，打破实体结构间的不可切割的障碍，使用户可以比原本的配置更好的方式来应用这些电脑硬件资源。这些资源的新虚拟部分是不受现有资源的架设方式，地域或物理配置所限制。一般所指的虚拟化资源包括**计算(CPU+内存），网络，存储**。

### 虚拟化的本质

**虚拟化**本质是指资源的抽象化，要想资源充分利用，必须把资源最小单位化(池化)，这样上层才能按需使用资源，虚拟化不但解放了操作系统，也解放了物理硬件，大大提高了**资源的利用率**。

![img](https://pic1.zhimg.com/80/v2-59292842f1143627a510b77822900bb8_720w.jpg)

- 虚拟化管理程序**Hypervisor**（VMM），位于虚拟机与底层硬件设备之间的虚拟层，直接运行于硬件设备之上，负责对硬件资源进行抽象，为上层虚拟机提供运行环境所需资源，并使每个虚拟机都能够互不干扰、相互独立地运行于同一个系统中。

## 二、虚拟化分类

虚拟化主要分为几大类：

- **计算虚拟化**，针对CPU和内存资源虚拟化技术。
- **IO虚拟化**，针对IO资源虚拟化技术。
- **网络虚拟化**，针对网络链路资源虚拟化技术。
- **存储虚拟化**，针对磁盘存储资源虚拟化技术。

### 1、计算虚拟化

**计算虚拟化**通过虚拟化管理程序（Hypervisor或VMM）将物理服务器的硬件资源与上层应用进行解耦，形成统一的计算资源池，然后可弹性分配给逻辑上隔离的虚拟机共享使用。如图2、基于VMM所在位置与虚拟化范围可以分三种类型。

![img](https://pic1.zhimg.com/80/v2-38396e1fad68c53c211cb2382642e1fc_720w.jpg)

![img](https://pic2.zhimg.com/80/v2-201f2233e1682b6bbeb75e4fc56a13e9_720w.jpg)

**容器（应用级）**：容器是一种更加轻量的应用级虚拟化技术，将应用的可执行文件及其所需的运行时环境与依赖库打包，实现一次构建，到处运行的目标。相比虚拟化，容器技术多了容器引擎层（如Docker），但上层应用无需与Guest OS绑定，可以实现秒级部署、跨平台迁移，灵活的资源分配，弹性调度管理等优势。容器、微服务与DevOps为云原生的三大要素，是推动企业技术中台建设与微服务化转型不可或缺的组件。

#### 实现-KVM

**KVM**是基于虚拟化扩展（Intel VT 或者 AMD-V）的 X86 硬件的开源的 Linux 原生的全虚拟化解决方案。KVM 中，虚拟机被实现为常规的 Linux 进程，由标准 Linux 调度程序进行调度；虚机的每个虚拟 CPU 被实现为一个常规的 Linux 进程。这使得 KMV 能够使用 Linux 内核的已有功能。

但是，KVM 本身不执行任何硬件模拟，需要客户空间程序通过 /dev/kvm 接口设置一个客户机虚拟服务器的地址空间，向它提供模拟的 I/O，并将它的视频显示映射回宿主的显示屏。目前这个应用程序是 QEMU。

![img](https://pic3.zhimg.com/80/v2-17a4f6eae217bc6a5c79c1fabcd33f3a_720w.jpg)

- **Guest**：客户机系统，包括CPU（vCPU）、内存、驱动（Console、网卡、I/O 设备驱动等），被 KVM 置于一种受限制的 CPU 模式下运行。
- **KVM**：运行在内核空间，提供CPU 和内存的虚级化，以及客户机的 I/O 拦截。Guest 的 I/O 被 KVM 拦截后，交给 QEMU 处理。
- **QEMU**：修改过的为 KVM 虚机使用的 QEMU 代码，运行在用户空间，提供硬件 I/O 虚拟化，通过 IOCTL /dev/kvm 设备和 KVM 交互。

**KVM**依赖的Intel/AMD 处理器的各种**虚拟化扩展**：

| 处理器 | CPU 虚拟化 | 内存虚拟化 | PCI Pass-through |
| ------ | ---------- | ---------- | ---------------- |
| Intel  | VT-x       | VPID，EPT  | VT-d             |
| AMD    | AMD-V      | ASID，NPT  | IOMMU            |

#### 内存虚拟化

![img](https://pic1.zhimg.com/80/v2-bbe049da6dc1e6cc0025588780440134_720w.jpg)

除了CPU虚拟化，另一个关键是**内存虚拟化**，通过内存虚拟化共享物理系统内存，动态分配给虚拟机。虚拟机的内存虚拟化很象现在的操作系统支持的虚拟内存方式，应用程序看到邻近的内存地址空间，这个地址空间无需和下面的物理机器内存直接对应，操作系统保持着虚拟页到物理页的映射。

**实现**

现在所有的 x86 CPU 都包括了一个称为内存管理的模块**MMU**（Memory Management Unit）和 **TLB**(Translation Lookaside Buffer)，通过MMU和TLB来优化虚拟内存的性能。

**KVM**中，虚机的物理内存即为 qemu-kvm 进程所占用的内存空间。KVM 使用 CPU 辅助的内存虚拟化方式。在 Intel 和 AMD 平台，其内存虚拟化的实现方式分别为：

- AMD 平台上的 **NPT** （Nested Page Tables） 技术
- Intel 平台上的 **EPT** （Extended Page Tables）技术

**EPT 和 NPT**采用类似的原理，都是作为 CPU 中新的一层，用来将客户机的物理地址翻译为主机的物理地址。关于 EPT， Intel 官方文档中的技术如下:

![img](https://pic2.zhimg.com/80/v2-580d74d4ebb546a2548d947cd99057d9_720w.jpg)

EPT的好处是，它的两阶段记忆体转换，特点就是将 Guest Physical Address → System Physical Address，VMM不用再保留一份 SPT (Shadow Page Table)，以及以往还得经过 SPT 这个转换过程。除了降低各部虚拟机器在切换时所造成的效能损耗外，硬体指令集也比虚拟化软体处理来得可靠与稳定。

### 2、I/O虚拟化

I/O虚拟化（Input/output virtualization，简称IOV）是虚拟化的一种新形式，是来自物理连接或物理运输上层协议的抽象，让物理服务器和虚拟机可以共享I/O资源。

**I/O虚拟化实现**

![img](https://pic4.zhimg.com/80/v2-92f2704b8fc6a9a93655ec2d2032e8f7_720w.jpg)

![img](https://pic2.zhimg.com/80/v2-d2f74059c1416a11ed248af7cc141189_720w.jpg)

**I/O 虚拟化**方案的选择：

- I/O设备尽量使用准虚拟化（virtio 和 vhost_net）
- 如果需要实时迁移，不能使用 SR-IOV
- 对更高I/O要求又不需要实时迁移的，可以使用 SR-IOV
- 每种方案都有优势和不足，在特定环境下其性能有可能反而下降，因此在生产环境中使用各种虚拟化方式前需要经过完整测试

### 3、网络虚拟化

![img](https://pic2.zhimg.com/80/v2-844d3a638326785c5d3d1a72275c2fdd_720w.jpg)

网络虚拟化 (NV) 是指将传统上在硬件中交付的网络资源抽象化到软件中。NV 可以将多个物理网络整合为一个基于软件的虚拟网络，或者可以将一个物理网络划分为多个隔离和独立的虚拟网络。

![img](https://pic4.zhimg.com/80/v2-5cc5a276b830ef22d7f5d2fe329d4613_720w.jpg)

#### 传统网络

- 在传统网络环境中，一台物理主机包含一个或多个网卡（NIC），要实现与其他物理主机之间的通信，需要通过自身的 NIC 连接到外部的网络设施，如交换机上。
- 这种架构下，为了对应用进行隔离，往往是将一个应用部署在一台物理设备上，这样会存在两个问题，
  1）是某些应用大部分情况可能处于空闲状态，
  2）是当应用增多的时候，只能通过增加物理设备来解决扩展性问题。不管怎么样，这种架构都会对物理资源造成极大的浪费。

![img](https://pic1.zhimg.com/80/v2-8d71a5b9f8e86d8ee0f9d69cc9b11dc4_720w.jpg)

其中虚拟机与虚拟机之间的通信，由虚拟交换机完成，虚拟网卡和虚拟交换机之间的链路也是虚拟的链路，整个主机内部构成了一个虚拟的网络，如果虚拟机之间涉及到三层的网络包转发，则又由另外一个角色——虚拟路由器来完成。

#### 虚拟化网络实现

**虚拟交换机-OVS**

**Open vSwitch** 是在开源 Apache 2 许可下获得许可的多层软件交换机。我们的目标是实现一个生产质量交换平台，该平台支持标准管理接口，并将转发功能开放给程序化扩展和控制。非常适合用作 VM 环境中的虚拟交换机。除了向虚拟网络层公开标准控制和可见性接口外，它还旨在支持跨多个物理服务器的分布。支持多种基于 Linux 的虚拟化技术，包括 Xen/XenServer、KVM 和 VirtualBox.

![img](https://pic2.zhimg.com/80/v2-fdac12ab32a02b00ff00b55cf450e7b9_720w.jpg)

- `datapath`是负责数据交换的内核模块，其从网口读取数据，并快速匹配Flowtable中的流表项，成功的直接转发，失败的上交`vswitchd`处理。它在初始化和`port binding`的时候注册钩子函数，把端口的报文处理接管到内核模块。
- `vswitchd`是一个守护进程，是ovs的管理和控制服务，通过unix socket将配置信息保存到`ovsdb`，并通过`netlink`和内核模块交互。
- `ovsdb`则是`ovs`的数据库，保存了`ovs`配置信息。

#### 虚拟网络实现技术-Overlay

当前主流overlay技术是 GRE 和 VXLAN技术. 通过增加扩展报文头来实现虚拟网络在物理网络之上传输报文。

**GRE**

![img](https://pic3.zhimg.com/80/v2-3402328fe1d33c13ab11228351b884aa_720w.jpg)

网络虚拟化使用通用路由封装 (NVGRE) 作为虚拟化 IP 地址的机制。在 NVGRE 中，虚拟机的数据包封装在另一个数据包中。此新 NVGRE 格式数据包的标头具有相应的源和目标提供程序区域 (PA) IP 地址。此外，它还具有 VSID (24 位虚拟子网 ID) ，该 ID 存储在新数据包的 GRE 标头中。

#### VXLAN

![img](https://pic1.zhimg.com/80/v2-36a25fc5d022c345781721f3303fe250_720w.jpg)

Virtual eXtensible Local Area Network (VXLAN) 是一种将2层报文封装到UDP包(Mac in UDP)中进行传输的一种封装协议。VXLAN主要是由Cisco推出的，VXLAN的包头有一个24bit的ID段，即意味着1600万个独一无二的虚拟网段，这个ID通常是对UDP端口采取伪随机算法而生成的（UDP端口是由该帧中的原始MAC Hash生成的）。这样做的好处是可以保证基于5元组的负载均衡，保存VM之间数据包的顺序。

#### 容器网络实现

![img](https://pic4.zhimg.com/80/v2-cee5dd08d160d231963390147116992b_720w.jpg)

CNI(Container Network Interface) 是 google 和 CoreOS 主导制定的容器网络标准，它 是在 RKT 网络提议 的基础上发展起来的，综合考虑了灵活性、扩展性、IP分配、多网卡等因素。CNI旨在为容器平台提供网络的标准化。不同的容器平台（比如目前的 Kubernetes、Mesos 和 RKT）能够通过相同的接口调用不同的网络组件。这个协议连接了两个组件：容器管理系统和网络插件，具体的事情都是插件来实现的，包括：创建容器网络空间（network namespace）、把网络接口（interface）放到对应的网络空间、给网络接口分配 IP 等。

Kubernetes本身并不负责网络通信，Kubernetes提供了容器网络接口CNI（Container Network Interface），具体的网络通信交给CNI插件来负责，开源的CNI插件非常多，像Flannel、Calico等

![img](https://pic2.zhimg.com/80/v2-be1e57af14dbaa20d61864d97ad7e941_720w.jpg)

### 4、存储虚拟化

存储虚拟化(Storage Virtualization)最通俗的理解就是对存储硬件资源进行抽象化表现。构建具有统一逻辑视图的存储资源池供用户按需使用。存储虚拟化将各个分散的存储系统 进行整合和统一管理，并提供了方便用户调用资源的接口。另外，存储虚拟化能够为后续的系统扩容提供便 利，使资源规模动态扩大时无需考虑新增的物理存储资源（如不同型号的存储设备）之间可能存在的差异。

![img](https://pic3.zhimg.com/80/v2-bcb99322aa9d827612ea94d87bd75ef2_720w.jpg)

#### 实现

![img](https://pic4.zhimg.com/80/v2-6474dbf08da427b311ef14261d200bc7_720w.jpg)

存储虚拟化的实现方式：
（1） 裸金属+逻辑卷
（2） 存储设备虚拟化
（3） 主机存储虚拟化+文件系统

#### 存储虚拟化分类

**文件、块和对象**是三种以不同的方式来保存、整理和呈现数据的存储格式。这些格式各有各的功能和限制。

- 文件存储会以文件和文件夹的层次结构来整理和呈现数据；
- 块存储会将数据拆分到任意划分且大小相同的卷中;
- 对象存储会管理数据并将其链接至关联的元数据。

![img](https://pic2.zhimg.com/80/v2-fbfeb29f939fb169a67a6f179836d2f1_720w.jpg)

- **块存储**：即提供裸的块设备服务，裸设备什么都没有，需要用户自己创建分区、创建文件系统、挂载到操作系统才能用，挂一个块存储设备到操作系统，相当于插一个新U盘。只实现了read、write、ioctl等接口。SAN、LVM、Ceph RBD、OpenStack Cinder等都属于块存储服务。
- **文件存储**：可以简单理解为分布式文件系统，通常实现了POSIX接口，不需要安装文件系统，直接像NFS一样挂载到操作系统就能用。典型的文件存储如NAS、HDFS、CephFS、GlusterFS、OpenStack Manila等。
- **对象存储**：提供Web存储服务，通过HTTP协议访问，只需要Web浏览器即可使用，不需要挂载到本地操作系统，实现的接口如GET、POST、DELETE等，典型的对象存储如百度网盘、S3、OpenStack Swift、Ceph RGW等。

## 虚拟化管理工具

**虚拟化管理工具**是指与虚拟化环境及背后的实体硬件对接的软件，它的作用是简化资源管理、分析数据并简化运维。每个虚拟化管理系统都各不相同，但大多数系统都会提供简单的用户界面，还能简化虚拟机（VM）创建流程、监控虚拟环境、分配资源、编译报告，以及自动执行规则。

![img](https://pic3.zhimg.com/80/v2-8748101e110adad4e009a44c9b43a35e_720w.jpg)

**libvirt**是一套用于管理硬件虚拟化的开源API、守护进程与管理工具。此套组可用于管理KVM、Xen、VMware ESXi、QEMU及其他虚拟化技术。libvirt内置的API广泛用于云解决方案开发中的虚拟机监视器编排层（Orchestration Layer）。