<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-Knowledge/AI/卷积算子实现">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.2.0">
<title data-rh="true">TensorCore 卷积算子实现原理 | Doongz&#x27;s Site</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://doongz.github.io/docs/Knowledge/AI/卷积算子实现"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="TensorCore 卷积算子实现原理 | Doongz&#x27;s Site"><meta data-rh="true" name="description" content="MegEngine TensorCore 卷积算子实现原理"><meta data-rh="true" property="og:description" content="MegEngine TensorCore 卷积算子实现原理"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://doongz.github.io/docs/Knowledge/AI/卷积算子实现"><link data-rh="true" rel="alternate" href="https://doongz.github.io/docs/Knowledge/AI/卷积算子实现" hreflang="en"><link data-rh="true" rel="alternate" href="https://doongz.github.io/docs/Knowledge/AI/卷积算子实现" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Doongz&#39;s Site RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Doongz&#39;s Site Atom Feed"><link rel="stylesheet" href="/assets/css/styles.99c6e2a1.css">
<link rel="preload" href="/assets/js/runtime~main.f9aa80d1.js" as="script">
<link rel="preload" href="/assets/js/main.0ded408c.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="My Site Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/img/logo.svg" alt="My Site Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Home page</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/intro">Notes</a><a class="navbar__item navbar__link" href="/community/support">Community</a><a class="navbar__item navbar__link" href="/blog">Blog</a><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Courses</a><ul class="dropdown__menu"><li><a href="https://github.com/doongz/mlc-ai" target="_blank" rel="noopener noreferrer" class="dropdown__link">Machine Learning Compilation<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li><a href="https://github.com/doongz/cs229" target="_blank" rel="noopener noreferrer" class="dropdown__link">Stanford CS229: Machine Learning<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li><a href="https://github.com/doongz/aics" target="_blank" rel="noopener noreferrer" class="dropdown__link">中国科学院 智能计算系统<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li><a href="https://github.com/doongz/cs50-ai" target="_blank" rel="noopener noreferrer" class="dropdown__link">Harvard CS50’s Introduction to AI with Python<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li><a href="https://github.com/doongz/mit-6.824" target="_blank" rel="noopener noreferrer" class="dropdown__link">MIT-6.824 Distributed Systems<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li><a href="https://github.com/doongz/os-workbench" target="_blank" rel="noopener noreferrer" class="dropdown__link">南京大学 操作系统：设计与实现<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li><a href="https://github.com/doongz/mit-6.s081" target="_blank" rel="noopener noreferrer" class="dropdown__link">MIT-6.S081 Operating Systems Engineering<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="navbar__items navbar__items--right"><a href="https://github.com/doongz" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebar_njMd"><nav class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/intro">Tutorial Intro</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Algorithm/前述/面试时做题技巧">Algorithm</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Competition/比赛">Competition</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Course/CMU-15-213">Course</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/Knowledge/IC/IC">Knowledge</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Knowledge/IC/IC">IC</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" tabindex="0" href="/docs/Knowledge/计算机组成/">计算机组成-序</a><button aria-label="Toggle the collapsible sidebar category &#x27;计算机组成-序&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Knowledge/操作系统/计算机系统概述">操作系统</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Knowledge/计算机网络/计算机网络体系结构">计算机网络</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Knowledge/容器技术/docker原理">容器技术</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/docs/Knowledge/数据存储/设计数据密集型应用/序">数据存储</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/docs/Knowledge/AI/Tensor">AI</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Knowledge/AI/Tensor">Tensor</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Knowledge/AI/算子开发">算子开发</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Knowledge/AI/卷积算子实现">TensorCore 卷积算子实现原理</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Knowledge/AI/深度学习模型编译技术">深度学习模型编译技术</a></li></ul></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Math/微积分/极限">Math</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Skill/ASM/RISC-V/汇编语言格式及ABI">Skill</a></div></li></ul></nav></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_OVgt"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Knowledge</span><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">AI</span><meta itemprop="position" content="2"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">TensorCore 卷积算子实现原理</span><meta itemprop="position" content="3"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>TensorCore 卷积算子实现原理</h1><p><a href="https://zhuanlan.zhihu.com/p/372973726" target="_blank" rel="noopener noreferrer">MegEngine TensorCore 卷积算子实现原理</a></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="一前言">一、前言<a class="hash-link" href="#一前言" title="Direct link to heading">​</a></h2><p>2020 年 5 月 Nvidia 发布了新一代的 GPU 架构安培（Ampere）。其中和深度学习关系最密切的莫过于性能强劲的第三代的 TensorCore ，新一代的 TensorCore 支持了更为丰富的 DL（Deep Learning）数据类型，包括了新的 TesorFloat-32（TF32），Bfloat16（BF16）计算单元以及 INT8, INT4 和 INT1 的计算单元，这些计算单元为 DL 推理提供了全面的支持。</p><p><strong>为了发挥这些计算单元的能力，以往会由资深的 HPC 工程师手写 GPU 汇编实现的卷积、矩阵乘算子来挖掘硬件的能力。然而凭借人力手工优化算子的方式已经没有办法应对如此多的数据类型，因此对于 DL 应用的优化渐渐地越来越依赖一些自动化的工具，例如面向深度学习领域的编译器</strong>。</p><p>在这样的趋势下， Nvidia 开发了线性代数模板库 CUTLASS ，抽象了一系列高性能的基本组件，可以用于生成各种数据类型，各种计算单元的卷积、矩阵乘算子。 MegEngine 在 CUTLASS 的基础上进行了二次开发，可以高效地开发新的高性能的算子，快速地迁移到新的 GPU 架构。在上一篇 <a href="https://zhuanlan.zhihu.com/p/258931422" target="_blank" rel="noopener noreferrer">文章</a> 中，我们已经简单介绍了 MegEngine 的底层卷积算子实现的使用方法，而本文将会深入介绍 MegEngine CUDA 平台的底层卷积算子的实现原理，并将会对 Nvidia CUTLASS 的 Implicit GEMM 卷积 <a href="https://link.zhihu.com/?target=https%3A//github.com/NVIDIA/cutlass/blob/master/media/docs/implicit_gemm_convolution.md" target="_blank" rel="noopener noreferrer">文档</a> 进行解读和补充。</p><p>因此，读者在阅读本文之前必须要了解的 CUDA 知识有：</p><ul><li><p>访问全局存储（Global Memory）时，同一 Warp 中的相邻线程访问连续的地址，访存请求会被合并，合并的访存能够最大化 Global Memory 的吞吐。</p></li><li><p>访问 Global Memory 时，尽可能使用最宽的数据类型（float4）进行访问，这样可以最大化访存指令的利用率。</p></li><li><p>CUDA 的共享存储（Shared Memory）按照每 4Bytes 划分为一个 bank，共分为 32 个 bank。当同一 Warp 中的线程访问同一 bank 的不同地址时会发生冲突（bank conflict）。无 bank conflict 的访存模式才能最大化 Shared Memory 的吞吐。</p></li><li><p>GPU 有显存（Global Memory）、L2、L1（Shared Memory）、寄存器 4 个层次的存储，直接访问显存的延迟很高，在优化 GEMM、Convolution 这样的计算密集型算子时，需要</p></li><li><ul><li>通过 L1 和寄存器的缓存来减少 Global Memory 的访存请求。</li><li>通过大量的计算来隐藏不可避免的 Global Memory 访存延迟。</li></ul></li></ul><p>首先，我们需要了解 CUTLASS 引入的一些抽象概念</p><ul><li><code>TileIterator</code> : 用于访问存储中的一个Tile的数据。<code>TileIterator</code> 实现了<code>advance()</code>方法，支持在 <code>Matrix</code> , <code>Tensor</code> 等数据类型上进行遍历。</li><li><code>Fragment</code> : 数组类型，用于存放 <code>TileIterator</code> 读取进来的数据。 <code>Fragment</code> 的数据通常存放在寄存器中。</li></ul><p>然后我们简单回顾一下 CUTLASS 设计的高性能的 GEMM 算子的 Pipeline，按照 Pipeline 实现的算子能够在 CUDA 平台上达到 cublas 的 90% 以上的性能。下图演示了 CUTLASS 设计的 Pipeline 化的 GEMM 算子：</p><p><img loading="lazy" src="https://pic2.zhimg.com/80/v2-bca194928d63c2948b78e14d9300efd1_720w.jpg" alt="img" class="img_ev3q"></p><ol><li>图中第一行演示了由 <code>PredicatedTileIterator</code> 和 <code>SmemTileIterator</code> 配合完成从 Global Memory 到 Shared Memory 的数据搬运。</li><li>第二行演示了 <code>WarpTileIterator</code> 负责从 Shared Memory 搬运数据到 <code>Fragment</code> 寄存器中。</li><li>第三行展示了<code>WarpMmaOperator</code> 用 <code>Fragment</code> 寄存器中的矩阵数据执行矩阵乘加 (Matrix-Multiply-Add) 操作。</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="二implicit-gemm-算法">二、Implicit GEMM 算法<a class="hash-link" href="#二implicit-gemm-算法" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="1卷积映射为矩阵乘法">1、卷积映射为矩阵乘法<a class="hash-link" href="#1卷积映射为矩阵乘法" title="Direct link to heading">​</a></h3><p>我们首先来看一下前向卷积算子的定义，假设输入的 feature map 是 x，卷积层的 weight 是 w，输出是 y，其中 x,y,w 都是 4 维的 Tensor，x 的四个维度分别是 NxICxIHxIW，w 的四个维度分别是 OCxICxFHxFW，y 的四个维度分别是 NxOCxOHxOW。那么输出 y 和输入 x, w 的数学关系式可以写成</p><p><img loading="lazy" src="https://www.zhihu.com/equation?tex=%5Ctext%7By%7D%28+%5Ctext%7Bn%7D%2C+%5Ctext%7Boc%7D%2C+%5Ctext%7Boh%7D%2C+%5Ctext%7Bow%7D+%29+%3D+%5Csum_%7B%5Ctext%7Bic%7D%7D+%5Csum_%7B%5Ctext%7Bfh%7D%7D+%5Csum_%7B%5Ctext%7Bfw%7D%7D+%5Ctext%7Bx%7D+%28%5Ctext%7Bn%7D%2C+%5Ctext%7Bic%7D%2C+%5Ctext%7Bih%7D%2C+%5Ctext%7Biw%7D%29+%5Ccdot+%5Ctext%7Bw%7D+%28+%5Ctext%7Boc%7D%2C+%5Ctext%7Bic%7D%2C+%5Ctext%7Bfh%7D%2C+%5Ctext%7Bfw%7D+%29" alt="[公式]" class="img_ev3q"></p><p>公式里的小写字母代表了 Tensor 在每一维的坐标，其中 ih，iw 和 oh，ow，fh，fw 的关系式可以写为</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">ih </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> oh </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> stride_h </span><span class="token operator" style="color:#393A34">-</span><span class="token plain"> pad_h </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> fh</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">iw </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> ow </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> stride_w </span><span class="token operator" style="color:#393A34">-</span><span class="token plain"> pad_w </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> fw</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>这里的<code>stride_h</code>, <code>stride_w</code>, <code>pad_h</code>, <code>pad_w</code>是卷积层的参数。
根据 im2col 算法的原理，公式里定义的卷积运算可以转化为一个矩阵乘法，也即</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">C </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> Matmul</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">A</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> B</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>其中</p><ul><li>矩阵 A 由 weight 转化而来，是一个 <img loading="lazy" src="https://www.zhihu.com/equation?tex=%5Ctext%7BOC%7D%5Ctimes%5Ctext%7BIC%7D%5Ccdot%5Ctext%7BFH%7D%5Ccdot%5Ctext%7BFW%7D" alt="[公式]" class="img_ev3q"> 的矩阵。</li><li>矩阵 B 由 feature map 转化而来，是一个 <img loading="lazy" src="https://www.zhihu.com/equation?tex=%5Ctext%7BIC%7D%5Ccdot%5Ctext%7BFH%7D%5Ccdot%5Ctext%7BFW%7D%5Ctimes%5Ctext%7BN%7D%5Ccdot%5Ctext%7BOH%7D%5Ccdot%5Ctext%7BOW%7D" alt="[公式]" class="img_ev3q"> 的矩阵</li><li>矩阵 C 代表了输出的 Tensor y，是一个 <img loading="lazy" src="https://www.zhihu.com/equation?tex=%5Ctext%7BOC%7D%5Ctimes%5Ctext%7BN%7D%5Ccdot%5Ctext%7BOH%7D%5Ccdot%5Ctext%7BOW%7D" alt="[公式]" class="img_ev3q"> 的矩阵。</li></ul><p>矩阵和 Tensor 在各个位置上的元素的对应关系为</p><p><img loading="lazy" src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D++%5Cbegin%7Baligned%7D+A_%7Bik%7D+%26%3D+%5Ctext%7Bw%7D%5Cleft%28%5Ctext%7Boc%7D%2C+%5Ctext%7Bic%7D%2C+%5Ctext%7Bfh%7D%2C+%5Ctext%7Bfw%7D%5Cright%29+%5Cend%7Baligned%7D+%5Cend%7Bequation%7D++" alt="[公式]" class="img_ev3q"></p><p><img loading="lazy" src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D+%5Cbegin%7Baligned%7D%5C+B_%7Bkj%7D+%26%3D+%5Ctext%7Bx%7D%5Cleft%28%5Ctext%7Bn%7D%2C+%5Ctext%7Bic%7D%2C+%5Ctext%7Bih%7D%2C+%5Ctext%7Biw%7D%5Cright%29+%5Cend%7Baligned%7D+%5Cend%7Bequation%7D+" alt="[公式]" class="img_ev3q"></p><p><img loading="lazy" src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D+%5Cbegin%7Baligned%7DC_%7Bij%7D+%26%3D+%5Ctext%7By%7D%5Cleft%28%5Ctext%7Bn%7D%2C+%5Ctext%7Boc%7D%2C+%5Ctext%7Boh%7D%2C+%5Ctext%7Bow%7D%5Cright%29+%5Cend%7Baligned%7D+%5Cend%7Bequation%7D" alt="[公式]" class="img_ev3q"></p><p>其中矩阵的下标 <img loading="lazy" src="https://www.zhihu.com/equation?tex=i%2C+j%2C+k" alt="[公式]" class="img_ev3q">和 Tensor 的坐标之间的关系为</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">i </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> oc</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">j </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> n </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> OH </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> OW </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> oh </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> OW </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> ow</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">k </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> ic </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> FH </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> FW </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> fh </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> FW </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> fw</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>当<img loading="lazy" src="https://www.zhihu.com/equation?tex=j" alt="[公式]" class="img_ev3q">已知时，可以用下面的关系式推算出 feature map 的坐标</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">n </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> j </span><span class="token operator" style="color:#393A34">/</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">OH </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> OW</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">j_res </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> j </span><span class="token operator" style="color:#393A34">%</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">OH </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> OW</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">oh </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> j_res </span><span class="token operator" style="color:#393A34">/</span><span class="token plain"> OW</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">ow </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> j_res </span><span class="token operator" style="color:#393A34">%</span><span class="token plain"> OW</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>当<img loading="lazy" src="https://www.zhihu.com/equation?tex=k" alt="[公式]" class="img_ev3q"> 已知时，可以推算出 weight 的坐标</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">ic </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> k </span><span class="token operator" style="color:#393A34">/</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">FH </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> FW</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">k_res </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> k </span><span class="token operator" style="color:#393A34">%</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">FH </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> FW</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">fh </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> k_res </span><span class="token operator" style="color:#393A34">/</span><span class="token plain"> FW</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">fw </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> k_res </span><span class="token operator" style="color:#393A34">%</span><span class="token plain"> FW</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>同时结合 oh, ow, fh, fw，就可以计算出 ih 和 iw。
根据上面的讨论，我们可以把卷积的运算过程，写成一个隐式矩阵乘法 (Implicit GEMM) 的形式：</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">GEMM_M </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> OC</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">GEMM_N </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> N </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> OH </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> OW</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">GEMM_K </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> IC </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> FH </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> FW</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> i </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">GEMM_M</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    oc </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> i</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> j </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">GEMM_N</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        accumulator </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        n </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> j </span><span class="token operator" style="color:#393A34">/</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">OH </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> OW</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        j_res </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> j </span><span class="token operator" style="color:#393A34">%</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">OH </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> OW</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        oh </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> j_res </span><span class="token operator" style="color:#393A34">/</span><span class="token plain"> OW</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        ow </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> j_res </span><span class="token operator" style="color:#393A34">%</span><span class="token plain"> OW</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> k </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">GEMM_K</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            ic </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> k </span><span class="token operator" style="color:#393A34">/</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">FH </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> FW</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            k_res </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> k </span><span class="token operator" style="color:#393A34">%</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">FH </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> FW</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            fh </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> k_res </span><span class="token operator" style="color:#393A34">/</span><span class="token plain"> FW</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            fw </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> k_res </span><span class="token operator" style="color:#393A34">%</span><span class="token plain"> FW</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            ih </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> oh </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> stride_h </span><span class="token operator" style="color:#393A34">-</span><span class="token plain"> pad_h </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> fh</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            iw </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> ow </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> stride_w </span><span class="token operator" style="color:#393A34">-</span><span class="token plain"> pad_w </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> fw</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            accumulator </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> accumulator </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> x</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">n</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> ic</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> ih</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> iw</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> w</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">oc</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> ic</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> fh</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> fw</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        y</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">n</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> oc</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> oh</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> ow</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> accumulator</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>上面的 Implicit GEMM 算法仍然是串行的形式，接下来我们要把它改造成 CUDA 上的并行算法。首先我们对整个计算任务进行分块，让每个线程块负责计算并输出大小为<code>TILE_MxTILE_N</code>的矩阵。于是算法变成了下面的形式：</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> i_out </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">GEMM_M </span><span class="token operator" style="color:#393A34">/</span><span class="token plain"> TILE_M</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> j_out </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">GEMM_N </span><span class="token operator" style="color:#393A34">/</span><span class="token plain"> TILE_N</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        ThreadblockConvolution</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> w</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> y</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">ThreadblockConvolution</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> w</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> y</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    accumulate</span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">TILE_M</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> TILE_N</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> i_in </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">TILE_M</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        oc </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> i_out </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> TILE_M </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> i_in</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> j_in </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">TILE_N</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            j </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> j_out </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> TILE_N </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> j_in</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            n </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> j </span><span class="token operator" style="color:#393A34">/</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">OH </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> OW</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            j_res </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> j </span><span class="token operator" style="color:#393A34">%</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">OH </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> OW</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            oh </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> j_res </span><span class="token operator" style="color:#393A34">/</span><span class="token plain"> OW</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            ow </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> j_res </span><span class="token operator" style="color:#393A34">%</span><span class="token plain"> OW</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> k </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">GEMM_K</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                ic </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> k </span><span class="token operator" style="color:#393A34">/</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">FH </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> FW</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                k_res </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> k </span><span class="token operator" style="color:#393A34">%</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">FH </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> FW</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                fh </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> k_res </span><span class="token operator" style="color:#393A34">/</span><span class="token plain"> FW</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                fw </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> k_res </span><span class="token operator" style="color:#393A34">%</span><span class="token plain"> FW</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                ih </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> oh </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> stride_h </span><span class="token operator" style="color:#393A34">-</span><span class="token plain"> pad_h </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> fh</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                iw </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> ow </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> stride_w </span><span class="token operator" style="color:#393A34">-</span><span class="token plain"> pad_w </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> fw</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                accumulator</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">i_in</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> j_in</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> accumulator</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">i_in</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> j_in</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                        </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> x</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">n</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> ic</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> ih</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> iw</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> w</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">oc</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> ic</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> fh</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> fw</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            y</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">n</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> oc</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> oh</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> ow</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> accumulator</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">i_in</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> j_in</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>为了提高访存的效率，我们可以在<code>GEMM_K</code>这一维上也进行分块，每次将<code>TILE_MxTILE_K</code>的矩阵 A 和<code>TILE_KxTILE_N</code>的矩阵 B 缓存到 Shared Memory 里，避免重复的 Global Memory 访存。于是，算法就变成了如下形式：</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> i_out </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">GEMM_M </span><span class="token operator" style="color:#393A34">/</span><span class="token plain"> TILE_M</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> j_out </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">GEMM_N </span><span class="token operator" style="color:#393A34">/</span><span class="token plain"> TILE_N</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        ThreadblockConvolution</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> w</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> y</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">ThreadblockConvolution</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> w</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> y</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    accumulator</span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">TILE_M</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> TILE_N</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    smem_A</span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">TILE_M</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> TILE_K</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    smem_B</span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">TILE_K</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> TILE_N</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">0</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> i_in </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">TILE_M</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        oc </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> i_out </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> TILE_M </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> i_in</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> j_in </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">TILE_N</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            j </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> j_out </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> TILE_N </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> j_in</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            n </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> j </span><span class="token operator" style="color:#393A34">/</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">OH </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> OW</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            j_res </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> j </span><span class="token operator" style="color:#393A34">%</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">OH </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> OW</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            oh </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> j_res </span><span class="token operator" style="color:#393A34">/</span><span class="token plain"> OW</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            ow </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> j_res </span><span class="token operator" style="color:#393A34">%</span><span class="token plain"> OW</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> k_out </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">GEMM_K </span><span class="token operator" style="color:#393A34">/</span><span class="token plain"> TILE_K</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                load_tile_to_smem</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> A_smem</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                load_tile_to_smem</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">w</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> B_smem</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                WarpGemm</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">A_smem</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> B_smem</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> accumulator</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            y</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">n</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> oc</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> oh</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> ow</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> accumulator</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">i_in</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> j_in</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">WarpGemm</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">A_smem</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> B_smem</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> accumulator</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> k_in </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">TILE_K</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        accumulator</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">i_in</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> j_in</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> accumulator</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">i_in</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> j_in</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> A_smem</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">i_in</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> k_in</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> B_smem</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">k_in</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> j_in</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>因为我们可以直接复用 CUTLASS 里已经实现好了高性能的<code>WarpMmaOperator</code>，所以实现基于 Implicit GEMM 的卷积算子只需要</p><ul><li>适配<code>DeviceConvolution</code>、<code>KernelConvolution</code>和<code>ThreadblockConvolution</code>，支持传入 Tensor 类型和 Convolution Layer 的参数。</li><li>添加<code>PredicateTileIterator</code>支持读取 Tensor 的一个 Tile 的数据到 Shared Memory 中，并隐式地将读入的数据组织成矩阵的形式。</li><li>算法的 main loop 中直接调用<code>WarpTileIterator</code>从 Shared Memory 读取数据，然后由<code>WarpGemmOperator</code>完成 Warp-level 的 GEMM 运算。</li><li><code>EpilogueOperator</code>适配卷积算子，将 Accumulator 的数据写回 Global Memory 的 Tensor 中。</li></ul><p>接下来我们会以 INT8 数据类型的 TensorCore 卷积算子来介绍 MegEngine 底层的卷积实现，本文会重点介绍 2、3、4 是如何实现的，关于如何使用已经写好的卷积算子，可以参考之前的 <a href="https://zhuanlan.zhihu.com/p/258931422" target="_blank" rel="noopener noreferrer">文章</a>。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="2global-memory-数据布局layout">2、Global Memory 数据布局（Layout）<a class="hash-link" href="#2global-memory-数据布局layout" title="Direct link to heading">​</a></h3><p>为了最大化 TensorCore 类型的卷积算子的吞吐，MegEngine 使用了 128 位的 Global Memory 访存指令，因此在访问 Tensor 的数据的时候要求地址满足 128 位对齐。MegEngine 使用了 NCHW32 的格式来存储 Tensor，NCHW32 格式的特点为：</p><ul><li>Tensor 的通道维度按照 32 个 channel 进行分组，每 32 个 channel 连续的存放在存储中。</li><li>Tensor 的其余维度按照 W、H、C、N 的顺序地址变化由快到慢的存放在存储中。</li></ul><p>由于采用了 32 个通道对齐的存储格式，因此卷积 layer 要求输入和输出 feature map 的通道数都是 32 的倍数。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="3预处理访存偏移量">3、预处理访存偏移量<a class="hash-link" href="#3预处理访存偏移量" title="Direct link to heading">​</a></h3><p>MegEngine 的卷积实现在<code>GEMM_K</code>的维度上是按照 <img loading="lazy" src="https://www.zhihu.com/equation?tex=%28%5Ctext%7BIC%7D%2F32%29%5Ccdot+%5Ctext%7BFH%7D%5Ccdot+%5Ctext%7BFW%7D%5Ccdot32" alt="[公式]" class="img_ev3q"> 的顺序累加，写成伪代码的形式如下：</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kInterleaved </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">32</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> ic_out </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">IC</span><span class="token operator" style="color:#393A34">//</span><span class="token plain">kInterleaved</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> fh </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">FH</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> fw </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">FW</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">            </span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> ic_in </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">kInterleaved</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                </span><span class="token comment" style="color:#999988;font-style:italic"># do mma</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                </span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">.</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>如果写成一层循环，那么应该写成：</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">kInterleaved </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">32</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">for</span><span class="token plain"> k </span><span class="token keyword" style="color:#00009f">in</span><span class="token plain"> </span><span class="token builtin">range</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">GEMM_K</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    chw </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> k </span><span class="token operator" style="color:#393A34">//</span><span class="token plain"> kInterleaved</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    ic_in </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> k </span><span class="token operator" style="color:#393A34">%</span><span class="token plain"> kInterleaved</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    ic_out </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> chw </span><span class="token operator" style="color:#393A34">//</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">FH </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> FW</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    chw_res </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> chw </span><span class="token operator" style="color:#393A34">%</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">FH </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> FW</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    fh </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> chw_res </span><span class="token operator" style="color:#393A34">//</span><span class="token plain"> FW</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    fw </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> chw_res </span><span class="token operator" style="color:#393A34">%</span><span class="token plain"> FW</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    pointer </span><span class="token operator" style="color:#393A34">+=</span><span class="token plain"> ic_out </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> C_STRIDE </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> fh </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> H_STRIDE </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> fw </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> W_STRIDE</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token comment" style="color:#999988;font-style:italic"># do mma</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">.</span><span class="token punctuation" style="color:#393A34">.</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>可以看到在迭代过程中，如果直接计算指针的偏移量的话，会引入很多除法和求余运算。而在 CUDA 平台上，整数的除法和求余的开销是非常大的，因此我们将一些地址的偏移量在 host 端预先计算好，存到 kernel param 的 buffer 中，需要时从 constant memory 中直接读取地址，避免除法和求余运算。 对于每个线程来说，在主循环中指针移动的 offset 如下图所示：</p><p><img loading="lazy" src="https://pic4.zhimg.com/80/v2-ebe752b39a8e61aac53a6904ac463deb_720w.jpg" alt="img" class="img_ev3q"></p><p>如果地址的增量可以用<code>delta</code>来表示的话，那么<code>delta</code>是以<code>FH*FW</code>为周期的，即：</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">delta</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">step</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> TILE_K</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> delta</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">step </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> </span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">FH </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> FW</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> TILE_K</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>因此我们只需要大约 <img loading="lazy" src="https://www.zhihu.com/equation?tex=%5Ctext%7BO%7D%5Cleft%28%5Ctext%7BFH%7D%5Ccdot%5Ctext%7BFW%7D%5Cright%29" alt="[公式]" class="img_ev3q"> 的存储空间。其中地址偏移量的计算逻辑可以参考代码 <a href="https://link.zhihu.com/?target=https%3A//github.com/MegEngine/cutlass/blob/61ff64e3ab6ad05b5ce2f205216901e8d030013d/include/cutlass/convolution/threadblock/conv2d_tile_iterator_nt_src_fprop_precomp.h%23L76" target="_blank" rel="noopener noreferrer">conv2d_tile_iterator_nt_src_fprop_precomp.h</a>。由于 kernel param buffer 的大小为 4KB，我们用了大约 3KB 来存储地址的增量，所以 MegEngine 的卷积实现要求 Convolution Layer 的<code>FH*FW</code>的大小不能太大，但是一般情况下，3x3, 5x5, 7x7 的卷积都可以处理。Nvidia 官方实现的迭代顺序与本文介绍的略有不同：</p><ul><li>官方实现需要将<code>IC</code>补齐为<code>TILE_K</code>的倍数，这样在通道数较小时会浪费一些计算量。</li><li>官方实现的线程块在访问输入 feature map 的时候地址的跨度比较大，降低了访存的局部性，对 cache 不够友好。</li></ul><p>因此在性能方面，MegEngine 的实现会更有优势，而官方实现的优点是对 Convolution Layer 的参数没有太多限制，通用性更好。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="4warp-level-mmamatrix-multiply-add-指令">4、Warp-level Mma(Matrix-multiply-add) 指令<a class="hash-link" href="#4warp-level-mmamatrix-multiply-add-指令" title="Direct link to heading">​</a></h3><p>cuda10.2 引入了新的 Warp-level 的<code>mma</code>和<code>ldmatrix</code>指令，用户可以通过<code>mma</code>指令使用 TensorCore 来进行高速的矩阵乘加运算，通过<code>ldmatrix</code>精细地控制 Warp 给 TensorCore 喂数据。其中<code>mma</code>指令的用法如下：</p><div class="language-cpp codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-cpp codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">unsigned</span><span class="token plain"> A</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> B</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic">// input matrix fragment data</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">int</span><span class="token plain"> C</span><span class="token punctuation" style="color:#393A34">[</span><span class="token number" style="color:#36acaa">2</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> D</span><span class="token punctuation" style="color:#393A34">[</span><span class="token number" style="color:#36acaa">2</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"> </span><span class="token comment" style="color:#999988;font-style:italic">// accumulators</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">asm</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">volatile</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token string" style="color:#e3116c">&quot;mma.sync.aligned.m8n8k16.rol.col.satfinite.s32.s8.s8.s32 {%0,$1}, {%2}, {%3}, {%4,%5};\n&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;=r&quot;</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">D</span><span class="token punctuation" style="color:#393A34">[</span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;=r&quot;</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">D</span><span class="token punctuation" style="color:#393A34">[</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;r&quot;</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">A</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;r&quot;</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">B</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;r&quot;</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">C</span><span class="token punctuation" style="color:#393A34">[</span><span class="token number" style="color:#36acaa">0</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;r&quot;</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">C</span><span class="token punctuation" style="color:#393A34">[</span><span class="token number" style="color:#36acaa">1</span><span class="token punctuation" style="color:#393A34">]</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>这条指令的语义是由一个 Warp 的 32 个线程同步地完成 8x8x16 的矩阵乘加运算，它有三个输入操作数，其中参与矩阵乘法运算的分别是一个 8x16 的矩阵 A 和一个 16x8 的矩阵 B，这两个输入矩阵的数据分布在同一 Warp 的 32 个线程中。 矩阵 A 的布局如下图所示：</p><p><img loading="lazy" src="https://pic1.zhimg.com/80/v2-9b682575fc2d03896316cfb7c3e5ed30_720w.jpg" alt="img" class="img_ev3q"></p><ul><li>同一 Warp 中的 32 个线程分为 8 组，每组四个线程，负责读取 8x16 的矩阵中的一行。</li><li>每一组中的一个线程读取每一行中相邻的 4 个 int8 的数据，恰好填满一个 32 位的寄存器。</li></ul><p>类似的矩阵 B 的布局如下图所示：</p><p><img loading="lazy" src="https://pic2.zhimg.com/80/v2-b5e223bb250730b14763cfc78a66833d_720w.jpg" alt="img" class="img_ev3q"></p><ul><li>每 4 个线程一组，共分为 8 组，每组负责读取 16x8 的矩阵中的一列。</li><li>每一组中的一个线程负责读取一列中相邻的 4 个数据。</li></ul><p>参与累加运算的矩阵 C 和输出矩阵 D 的数据也同样分布在 32 个线程中，它们的布局如下图所示：</p><p><img loading="lazy" src="https://pic1.zhimg.com/80/v2-6c36ecfb5066dcc205bbcb92a9a56578_720w.jpg" alt="img" class="img_ev3q"></p><ul><li>同样每 4 个线程一组，每组负责读入/输出一行的数据。</li><li>每个线程负责输出一行中的相邻两个 int32 类型的数据，恰好构成一个 64 位的寄存器。</li></ul><p>通过对<code>mma</code>指令的分析，如果 Global Memory/Shared Memory 中的数据是以行优先 (RowMajor) 或者列优先 (ColumnMajor) 的格式存储的，那么当同一 Warp 执行空间上连续的两个 8x8x16 的矩阵乘加运算时，每个线程读取的数据将会是跳跃的，执行每次乘法都只能读取 32 位宽的数据到寄存器中，而低位宽的 Load 指令通常没有办法最大化利用存储的带宽。因此 Nvidia 提供了<code>ldmatrix</code>的指令，可以让同一 Warp 一次性读取 4 个 8x16 的矩阵到寄存器中，这样恰好可以让 Warp 中的每个线程一次读取 128 位的数据，最大化带宽的利用率。 <code>ldmarix</code>的用法如下所示：</p><div class="language-cpp codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-cpp codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">unsigned</span><span class="token plain"> addr</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic">// shared memory pointer</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">int</span><span class="token plain"> x</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> y</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> z</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> w</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"> </span><span class="token comment" style="color:#999988;font-style:italic">// loaded data</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">int4 data</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain">      </span><span class="token comment" style="color:#999988;font-style:italic">// loaded fragment</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">asm</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">volatile</span><span class="token punctuation" style="color:#393A34">(</span><span class="token string" style="color:#e3116c">&quot;ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%0, %1, %2, %3}, [%4];&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;=r&quot;</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;=r&quot;</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">y</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;=r&quot;</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">z</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;=r&quot;</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">w</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    </span><span class="token operator" style="color:#393A34">:</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;r&quot;</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">addr</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">data </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">make_int4</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> y</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> z</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> w</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>上述这条指令恰好读取了 4 个 8x16 的矩阵，每个线程恰好负责读取矩阵的一行数据，读取完成后，线程之间会进行数据交换，将矩阵的数据重新分布到各个线程，读取的过程如下图所示：</p><p><img loading="lazy" src="https://pic1.zhimg.com/80/v2-145e6d0b04b0fa86f4ba2036b4d42350_720w.jpg" alt="img" class="img_ev3q"></p><p>这一节介绍了 TensorCore 相关的<code>mma</code>和<code>ldmatrix</code>指令，有了这两条高性能的指令，我们还需要为数据设计巧妙的 Shared Memory 存储格式，消除从 Shared Memory 读取数据的 bank conflict，从而提升 Shared Memory 的读取效率。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="5shared-memory-的数据布局">5、Shared Memory 的数据布局<a class="hash-link" href="#5shared-memory-的数据布局" title="Direct link to heading">​</a></h3><p>在介绍 Shared Memory 中的数据布局之前，我们需要了解 Shared Memory 的访存特点。Shared Memory 按照每 4 个字节组成一个 bank，共划分成了 32 个 bank，同一 Warp 的线程访问了相同 bank 的不同地址时会发生 conflict，导致访存的效率变慢。在同一 Warp 的线程访问不同位宽的数据时，会有不同的行为：</p><ul><li><p>每个线程访问 Shared Memory 中 32 位的数据，访存将在一个阶段内完成。</p></li><li><p>每个线程访问 Shared Memory 中 64 位的数据，访存会在两个阶段内完成：</p></li><li><ul><li>第一个阶段：前 16 个线程访存 128 字节的数据。</li><li>第二个阶段：后 16 个线程访存 128 字节的数据。</li></ul></li><li><p>每个线程访问 Shared Memory 中的 128 位的数据，访存会在四个阶段内完成：</p></li><li><ul><li>每个阶段由 8 个线程完成 128 字节的数据的访存。</li></ul></li></ul><p>如果上述过程中每个阶段都没有 bank conflict，则能够达到最大的 Shared Memory 访存效率。 通常为了避免 Shared Memory 的 bank conflict，我们会对 Shared Memory 的数据进行 padding，让线程访问的数据错开，避免落在同一 bank 中。但是这样做的问题是会使得 kernel 需要 Shared Memory 的 Size 变大，但是 SM 上的 L1 cache(Shared Memory) 又是有限的，所以 padding 会降低 kernel 的 occupancy，进而就会降低 kernel 的性能。 因此 CUTLASS 设计了一种 Shared Memory 的交错布局方式，它能够在不进行 padding 的前提下，使得线程访存的地址没有 bank conflict。接下来，我们以 64x64 的矩阵为例来详细介绍数据在 Shared Memory 中的布局。首先，线程读取数据的粒度都是 128 位，也即 16 个 INT8 类型的数据，因此我们在演示数据的布局时总是以 16 个数据为一组。如果矩阵是以行优先 (RowMajor) 的格式来组织的，那么在逻辑上的布局如下图所示：</p><p><img loading="lazy" src="https://pic3.zhimg.com/80/v2-55e42c7cc835e35cd6df4dd2abe794fa_720w.jpg" alt="img" class="img_ev3q"></p><p>从图中可以看到</p><ul><li>每 16 个元素分为一组，被称为一个 Vector，被染上了不同的颜色。</li><li>每行相邻的 32 个元素被称为一个 Crosswise，恰好是 NCHW32 格式中的一组 channel 的数据。</li></ul><p>在 Shared Memory 的物理存储中，矩阵的数据进行了重新排列，如下图所示：</p><p><img loading="lazy" src="https://pic1.zhimg.com/80/v2-e5068ce83da288a16d2519c2fc85280c_720w.jpg" alt="img" class="img_ev3q"></p><p>我们可以看到 Shared Memory 的物理布局有以下特点：</p><ul><li>每 4 行的一个 Crosswise 的数据作为一组，连续存放在 Shared Memory 中，紧接着会存放这 4 行的下一个 Crosswise 的数据。</li><li>每组数据包含了 8 个 Vector，占据了 128 个字节，恰好是 Shared Memory 中的 32 个不同的 bank。</li><li>每组数据在排列是进行了交错，保证了<code>ldmatrix</code>时不会发生 bank conflict。</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="6显存---shared-memory-的数据搬运">6、显存 -&gt; Shared Memory 的数据搬运<a class="hash-link" href="#6显存---shared-memory-的数据搬运" title="Direct link to heading">​</a></h3><p>这一节我们会介绍从显存 (Global Memory) 到 Shared Memory 的数据搬运。显存到 Shared Memory 的数据搬运是由 <a href="https://link.zhihu.com/?target=https%3A//github.com/MegEngine/cutlass/blob/61ff64e3ab6ad05b5ce2f205216901e8d030013d/include/cutlass/convolution/threadblock/conv2d_tile_iterator_nt_src_fprop_precomp.h%23L346" target="_blank" rel="noopener noreferrer">Conv2dTileSrcIteratorFpropPrecomp</a> 来完成的，本文并不会详细地解读代码的实现，而是描述线程搬运数据的过程，帮助大家建立直观的印象，更好地理解代码。 如果以上一节中 Shared Memory 的逻辑布局为例，同一 Warp 中每个线程读取的数据的逻辑布局如下图所示，每个线程读取 16 个 INT8 类型的数据，恰好构成一个 Vector。</p><p><img loading="lazy" src="https://pic3.zhimg.com/80/v2-c550fcbebc078dadc60b82d91f6155c2_720w.jpg" alt="img" class="img_ev3q"></p><p>而在实际的物理显存中，线程访问的数据分布如下图所示：</p><p><img loading="lazy" src="https://pic4.zhimg.com/80/v2-3be99f8b44555cb49c60b2980a4b674b_720w.jpg" alt="img" class="img_ev3q"></p><ul><li>我们可以看到每个线程读取了 128 位的数据。</li><li>相邻的线程读取的数据在物理上是连续的。</li></ul><p>因此线程从 Global Memory 读取数据的 pattern 可以满足合并访存的要求，同时以最大的数据位宽进行访存，最大化了显存带宽的利用率。 然后如果将线程读取的数据映射到 Shared Memory 的物理地址，我们可以看到：</p><ul><li>每 8 个线程向 Shared Memory 写入 128 字节的数据，恰好落在 Shared Memory 的 32 个不同的 bank 中。</li><li>同一 Warp 的访存分为四个阶段完成，每个阶段都没有 bank conflict。</li></ul><p>下图演示了一个 Warp 写入 Shared Memory 的过程：</p><p><img loading="lazy" src="https://pic4.zhimg.com/80/v2-dcfa6ce2c070997f285520a3ce08c627_720w.jpg" alt="img" class="img_ev3q"></p><p><img loading="lazy" src="https://pic3.zhimg.com/80/v2-fc060f229b089461e22f991bc15817fa_720w.jpg" alt="img" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="7shared-memory---寄存器的数据搬运">7、Shared Memory -&gt; 寄存器的数据搬运<a class="hash-link" href="#7shared-memory---寄存器的数据搬运" title="Direct link to heading">​</a></h3><p>Shared Memory 到寄存器的数据搬运是由 <a href="https://link.zhihu.com/?target=https%3A//github.com/MegEngine/cutlass/blob/61ff64e3ab6ad05b5ce2f205216901e8d030013d/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator.h%23L1342" target="_blank" rel="noopener noreferrer">MmaTensorOpMultiplicandTileIterator</a> 完成的。同一 Warp 在每一轮迭代过程会读取 4 个 8x16 的矩阵到寄存器中，每个线程会读取一行的数据。例如第一轮迭代时，线程读取的数据在逻辑上的布局如下图所示：</p><p><img loading="lazy" src="https://pic3.zhimg.com/80/v2-fbde94d54faede0c63b6b9703038bae6_720w.jpg" alt="img" class="img_ev3q"></p><p>而实际上数据在 Shared Memory 里的物理布局如下图：</p><p><img loading="lazy" src="https://pic1.zhimg.com/80/v2-9b48dd9d905108af474ec0b06d01f7a0_720w.jpg" alt="img" class="img_ev3q"></p><p>可以看到：</p><ul><li>每个线程读取了 128 位的数据，因此访存分为四个阶段来进行。</li><li>每一阶段的 8 个线程读取的数据恰好落在了 Shared Memory 的 32 个 bank 中，并且线程访存的数据之间不存在冲突。</li></ul><p>当进行到第二轮迭代时，每个线程访问的数据的物理布局如下图：</p><p><img loading="lazy" src="https://pic4.zhimg.com/80/v2-6e0d89631210400c41227a909bc29f3b_720w.jpg" alt="img" class="img_ev3q"></p><p>同样的访存的每一个阶段都不存在 bank conflict。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="8accumulator-写回全局存储">8、Accumulator 写回全局存储<a class="hash-link" href="#8accumulator-写回全局存储" title="Direct link to heading">​</a></h3><p>在 int8 的情况下，同一 Warp 负责输出 64x64 的结果，kernel 会分成 8 次写回 Global Memory，每次写回 32x8 的矩阵。这样保证了每次将 Tensor 按照 NCHW32 格式写回显存时，同一 Warp 的 32 个线程恰好写了物理上连续的 256 字节的数据，而每个线程写回 8 个字节，保证了可以使用64位宽的数据类型进行显存的写操作，尽可能提高带宽的利用率。 由于<code>mma</code>指令的特点，输出矩阵的数据分布在各个线程上，而为了能够合并访存，即：让相邻线程写回的地址是连续的，我们利用 Shared Memory 对同一 Warp 中 32 个线程的数据进行了交换。数据交换后，每个线程拥有连续的 8 个通道的数据，且线程写的地址是连续的，保证了写回 Global Memory 满足合并访存的要求。 线程交换数据的过程如下图所示：</p><p><img loading="lazy" src="https://pic1.zhimg.com/80/v2-ded6dacd217ab74f358c04de3a49ea10_720w.jpg" alt="img" class="img_ev3q"></p><p>每一轮迭代，Warp 中的 32 个线程将 32x16 的矩阵数据写入到 Shared Memory 中。接着如下图所示，每个线程会把连续的 8 个 channel 的数据读到寄存器中。</p><p><img loading="lazy" src="https://pic1.zhimg.com/80/v2-16b278f2cf43ba871eec02f1d440596c_720w.jpg" alt="img" class="img_ev3q"></p><p>Shared Memory 的数据交换是由以下两个<code>Iterator</code>完成的</p><ul><li><a href="https://link.zhihu.com/?target=https%3A//github.com/MegEngine/cutlass/blob/61ff64e3ab6ad05b5ce2f205216901e8d030013d/include/cutlass/epilogue/warp/interleaved_tile_iterator_tensor_op.h%23L76" target="_blank" rel="noopener noreferrer">InterleavedTileIteratorTensorOp</a> 完成了每一轮迭代将 32x8 的数据写入到 Shared Memory 中。</li><li><a href="https://link.zhihu.com/?target=https%3A//github.com/MegEngine/cutlass/blob/61ff64e3ab6ad05b5ce2f205216901e8d030013d/include/cutlass/epilogue/threadblock/interleaved_shared_load_iterator_tensor_op.h%23L73" target="_blank" rel="noopener noreferrer">InterleavedSharedLoadIteratorTensorOp</a> 负责将连续的 8 个 channel 的数据读到<code>Fragment</code>寄存器中。</li></ul><p>当线程将交换后的数据读到<code>Fragment</code>寄存器之后，会由<code>EpilogueOp</code>，在卷积的基础上完成<code>BiasAdd</code>的运算。以 <a href="https://link.zhihu.com/?target=https%3A//github.com/MegEngine/cutlass/blob/61ff64e3ab6ad05b5ce2f205216901e8d030013d/include/cutlass/epilogue/thread/bias_add_linear_combination_relu.h%23L80" target="_blank" rel="noopener noreferrer">BiasAddLinearCombinationRelu</a> 为例，它实际上完成了下面的运算：</p><div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">accumulator </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> conv</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">x</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain"> w</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">y </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> alpha </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> accumulator </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> beta </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> bias </span><span class="token operator" style="color:#393A34">+</span><span class="token plain"> gamma </span><span class="token operator" style="color:#393A34">*</span><span class="token plain"> z</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>其中 bias 是一个<code>PerChannel</code>的 Tensor，代表了每个输出通道的偏置，z 是一个和卷积输出大小一致的 Tensor，用于<code>Convolution</code>和<code>ElemwiseAdd</code>的融合。 最后<code>EpilogueOp</code>的输出会由 <a href="https://link.zhihu.com/?target=https%3A//github.com/MegEngine/cutlass/blob/61ff64e3ab6ad05b5ce2f205216901e8d030013d/include/cutlass/epilogue/threadblock/tensor_predicated_tile_iterator_tensor_op.h%23L83" target="_blank" rel="noopener noreferrer">TensorPredicatedTileIteratorTensorOp</a> 真正地写回到 Global Memory 中。每个线程写回的数据如下图所示：</p><p><img loading="lazy" src="https://pic3.zhimg.com/80/v2-de08cd301f34d8084a5b60742e611c8e_720w.jpg" alt="img" class="img_ev3q"></p><p>可以看到线程写回的 pattern 满足合并访存的要求，因此能最大化 Global Memory 写的效率。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="三总结">三、总结<a class="hash-link" href="#三总结" title="Direct link to heading">​</a></h2><p>本文介绍了 MegEngine 底层的卷积算子实现原理，算子性能可以达到 cudnn 的 80% 以上，测速结果可以参见 <a href="https://zhuanlan.zhihu.com/p/258931422" target="_blank" rel="noopener noreferrer">文章</a>。</p><p>MegEngine 会对卷积实现进行持续优化，进一步提升算子的性能，目前来看有以下两点可做的优化：</p><ul><li><p>借鉴 Nvidia 官方 CUTLASS ImplicitGEMM Convolution 实现对 mask 的处理，提高<code>TileIterator</code>对于 mask 判断的效率。</p></li><li><p>现在的卷积实现在写回显存时利用 Shared Memory 进行数据交换是存在 bank conflict 的。后续会考虑两点优化</p></li><li><ul><li>对 Shared Memory 的数据布局进行探索，消除 bank conflict，优化 Shared Memory 数据交换的效率。</li><li>对 Global Memory 中的 Weight Tensor 的布局进行探索，提高每个 Thread 上 accumulator 的局部性，避免在 Shared Memory 中进行数据交换。</li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="参考资料">参考资料<a class="hash-link" href="#参考资料" title="Direct link to heading">​</a></h2><ul><li><a href="https://link.zhihu.com/?target=https%3A//docs.nvidia.com/cuda/parallel-thread-execution/index.html%23warp-level-matrix-fragment-mma-8816" target="_blank" rel="noopener noreferrer">Warp-level Matrix Fragment Mma PTX 文档</a></li><li><a href="https://link.zhihu.com/?target=https%3A//github.com/MegEngine/cutlass/blob/61ff64e3ab6ad05b5ce2f205216901e8d030013d/media/docs/implicit_gemm_convolution.md" target="_blank" rel="noopener noreferrer">CUTLASS Implicit GEMM Convolution 官方文档</a></li><li><a href="https://link.zhihu.com/?target=https%3A//on-demand.gputechconf.com/gtc/2018/presentation/s81006-volta-architecture-and-performance-optimization.pdf" target="_blank" rel="noopener noreferrer">Volta architecture and performance optimization</a></li><li><a href="https://link.zhihu.com/?target=https%3A//developer.download.nvidia.cn/video/gputechconf/gtc/2020/presentations/s21745-developing-cuda-kernels-to-push-tensor-cores-to-the-absolute-limit-on-nvidia-a100.pdf" target="_blank" rel="noopener noreferrer">Developing CUDA kernels to push Tensor Cores to the absolute limit on Nvidia A100</a></li></ul></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/Knowledge/AI/算子开发"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">算子开发</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/Knowledge/AI/深度学习模型编译技术"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">深度学习模型编译技术</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#一前言" class="table-of-contents__link toc-highlight">一、前言</a></li><li><a href="#二implicit-gemm-算法" class="table-of-contents__link toc-highlight">二、Implicit GEMM 算法</a><ul><li><a href="#1卷积映射为矩阵乘法" class="table-of-contents__link toc-highlight">1、卷积映射为矩阵乘法</a></li><li><a href="#2global-memory-数据布局layout" class="table-of-contents__link toc-highlight">2、Global Memory 数据布局（Layout）</a></li><li><a href="#3预处理访存偏移量" class="table-of-contents__link toc-highlight">3、预处理访存偏移量</a></li><li><a href="#4warp-level-mmamatrix-multiply-add-指令" class="table-of-contents__link toc-highlight">4、Warp-level Mma(Matrix-multiply-add) 指令</a></li><li><a href="#5shared-memory-的数据布局" class="table-of-contents__link toc-highlight">5、Shared Memory 的数据布局</a></li><li><a href="#6显存---shared-memory-的数据搬运" class="table-of-contents__link toc-highlight">6、显存 -&gt; Shared Memory 的数据搬运</a></li><li><a href="#7shared-memory---寄存器的数据搬运" class="table-of-contents__link toc-highlight">7、Shared Memory -&gt; 寄存器的数据搬运</a></li><li><a href="#8accumulator-写回全局存储" class="table-of-contents__link toc-highlight">8、Accumulator 写回全局存储</a></li></ul></li><li><a href="#三总结" class="table-of-contents__link toc-highlight">三、总结</a></li><li><a href="#参考资料" class="table-of-contents__link toc-highlight">参考资料</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/intro">Tutorial</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/questions/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/facebook/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2022 Doongz Inc. Built with Docusaurus.</div></div></div></footer></div>
<script src="/assets/js/runtime~main.f9aa80d1.js"></script>
<script src="/assets/js/main.0ded408c.js"></script>
</body>
</html>